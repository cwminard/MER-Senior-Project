{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7636b468",
   "metadata": {},
   "source": [
    "capture audio and video data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88853729",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import pyaudio\n",
    "import wave\n",
    "import time\n",
    "import threading\n",
    "\n",
    "def record_video_and_audio():\n",
    "    cap = cv.VideoCapture(0)\n",
    "    fourcc = cv.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv.VideoWriter('C:\\\\Users\\\\chels\\\\Desktop\\\\COSC490-MER\\\\outputs\\\\video\\\\output.mp4', fourcc, 20.0, (640, 480))\n",
    "\n",
    "    CHUNK = 4096  # Increased chunk size for smoother audio\n",
    "    FORMAT = pyaudio.paInt16\n",
    "    CHANNELS = 1\n",
    "    RATE = 44100\n",
    "    WAVE_OUTPUT_FILENAME = \"C:\\\\Users\\\\chels\\\\Desktop\\\\COSC490-MER\\\\outputs\\\\audio\\\\output.wav\"\n",
    "    frames = []\n",
    "\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = None\n",
    "\n",
    "    audio_started = False\n",
    "    audio_running = True\n",
    "\n",
    "    def audio_record():\n",
    "        nonlocal stream, frames, audio_running\n",
    "        stream = p.open(format=FORMAT,\n",
    "                        channels=CHANNELS,\n",
    "                        rate=RATE,\n",
    "                        input=True,\n",
    "                        frames_per_buffer=CHUNK)\n",
    "        print(\"* recording audio\")\n",
    "        while audio_running:\n",
    "            data = stream.read(CHUNK, exception_on_overflow=False)\n",
    "            frames.append(data)\n",
    "\n",
    "    audio_thread = threading.Thread(target=audio_record)\n",
    "    audio_thread.start()\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "            break\n",
    "        out.write(frame)\n",
    "        cv.imshow('frame', frame)\n",
    "\n",
    "        if cv.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Cleanup video\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv.destroyAllWindows()\n",
    "\n",
    "    # Cleanup audio\n",
    "    audio_running = False\n",
    "    audio_thread.join()\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "    wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "    wf.setnchannels(CHANNELS)\n",
    "    wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(b''.join(frames))\n",
    "    wf.close()\n",
    "    print(\"* done recording audio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4d4bb4",
   "metadata": {},
   "source": [
    "grab the transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "999172fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def transcribe_audio():\n",
    "  base_url = \"https://api.assemblyai.com\"\n",
    "\n",
    "  headers = {\n",
    "      \"authorization\": \"6a35340cac1c443e8e4bbc1d027a3ad5\"\n",
    "  }\n",
    "  with open(\"outputs/audio/output.wav\", \"rb\") as f:\n",
    "    response = requests.post(base_url + \"/v2/upload\",\n",
    "                            headers=headers,\n",
    "                            data=f)\n",
    "\n",
    "  audio_url = response.json()[\"upload_url\"]\n",
    "\n",
    "\n",
    "  data = {\n",
    "      \"audio_url\": audio_url,\n",
    "      \"speech_model\": \"universal\"\n",
    "  }\n",
    "\n",
    "  url = base_url + \"/v2/transcript\"\n",
    "  response = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "  transcript_id = response.json()['id']\n",
    "  polling_endpoint = base_url + \"/v2/transcript/\" + transcript_id\n",
    "\n",
    "  while True:\n",
    "    transcription_result = requests.get(polling_endpoint, headers=headers).json()\n",
    "    transcript_text = transcription_result['text']\n",
    "\n",
    "    if transcription_result['status'] == 'completed':\n",
    "      print(\"Transcription completed.\")\n",
    "      return transcript_text\n",
    "      # with open(\"transcript.txt\", \"w\") as file:\n",
    "      #   file.write(transcript_text) # we don't necessarily need to write it into a file\n",
    "      # file.close()\n",
    "\n",
    "    elif transcription_result['status'] == 'error':\n",
    "      raise RuntimeError(f\"Transcription failed: {transcription_result['error']}\")\n",
    "\n",
    "    else:\n",
    "      time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae605509",
   "metadata": {},
   "source": [
    "sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a611091f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "# analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# def tokens_to_text(tokens):\n",
    "#     text = \" \".join(tokens)\n",
    "#     score = analyzer.polarity_scores(text)\n",
    "#     return score\n",
    "\n",
    "def determine_sentiment(text):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    score = analyzer.polarity_scores(text)\n",
    "    if score['compound'] >= 0.05:\n",
    "        return 'positive'\n",
    "    elif score['compound'] <= -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b47126c",
   "metadata": {},
   "source": [
    "video analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de457621",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chels\\Desktop\\COSC490-MER\\.venv\\Lib\\site-packages\\fer\\fer.py:37: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n",
      "c:\\Users\\chels\\Desktop\\COSC490-MER\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from fer.fer import FER\n",
    "from fer.classes import Video\n",
    "import pandas as pd\n",
    "def analyze_video_emotions():\n",
    "    emotion_detector = FER(mtcnn=True)\n",
    "    video = Video(\"C:\\\\Users\\\\chels\\\\Desktop\\\\COSC490-MER\\\\outputs\\\\video\\\\output.mp4\")\n",
    "\n",
    "\n",
    "    emotions = video.analyze(emotion_detector, display=False, frequency=15)\n",
    "    df = pd.DataFrame(emotions)\n",
    "\n",
    "\n",
    "    stats = df.describe()\n",
    "    emotions = stats.loc['mean'].nlargest(2).index.tolist()\n",
    "    return emotions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bd7be6",
   "metadata": {},
   "source": [
    "therapyAI chatbot response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dc9d07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "def chatbot_response(emotions, sentiment, text):\n",
    "  response: ChatResponse = chat(model='gemma3', messages=[\n",
    "    {\n",
    "      'role': 'system',\n",
    "      'content': f'Based on the user\\'s two primary emotions {emotions[0]} and {emotions[1]} from the video analysis, and based on the sentiment of the text which is {sentiment}, generate a therapuetic, empathetic response to the user\\'s words. Avoid asking follow-up questions (if waranted), but rather provide advice as a therapist would. The user said: {text}'\n",
    "    }\n",
    "  ])\n",
    "  # or access fields directly from the response object\n",
    "  print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a20c65",
   "metadata": {},
   "source": [
    "final call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91c8eb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* recording audio\n",
      "* done recording audio\n",
      "Transcription completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:py.warnings:c:\\Users\\chels\\Desktop\\COSC490-MER\\.venv\\Lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n",
      "\n",
      "INFO:fer:20.00 fps, 1047 frames, 52.35 seconds\n",
      "INFO:fer:Making directories at output\n",
      "INFO:fer:Deleted pre-existing output\\output_output.mp4\n",
      "INFO:fer:Async I/O enabled for frame saving\n",
      "70frames [00:09,  7.00frames/s]                    \n",
      "INFO:fer:Waiting for async I/O to complete...\n",
      "INFO:fer:Async I/O completed\n",
      "INFO:fer:Completed analysis: saved to output\\output_output.mp4\n",
      "INFO:fer:Starting to Zip\n",
      "INFO:fer:Compressing: 71%\n",
      "INFO:fer:Zip has finished\n",
      "INFO:httpx:HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It sounds like you’re carrying a significant load right now – juggling finals, student organizations, and that feeling of pressure. It's incredibly wise of you to recognize this and proactively seek ways to manage it, especially given the mix of happiness you’re feeling alongside the stress. It’s completely understandable to feel overwhelmed when so much is happening at once. \n",
      "\n",
      "Allowing yourself to acknowledge a decent day, even while recognizing the underlying stress, is a really important step.  Focusing on small, manageable steps – like exploring coping strategies – is a powerful way to regain a sense of control. Don’t underestimate the value of acknowledging and validating your feelings; it’s a cornerstone of healthy stress management.  \n",
      "\n",
      "Try to carve out even just fifteen minutes a day for something that brings you a little joy, and remember, taking care of yourself during this busy time isn’t a luxury, it’s essential for your well-being and ability to navigate these challenges effectively.\n",
      "TherapyAI Response: None\n",
      "Sentiment: positive\n",
      "Transcribed Text: So it opens a camera and it'll. I'm just gonna say, like, oh, I had a decent day today. I'm kind of stressed out because finals are coming up and I'm just trying to find coping strategies to help balance my life. I have a lot going on with student organizations and other elements of life, so how can I cope with that? It's a lot of pressure on me, and although I am happy, I'm a bit concerned about where I'm at currently and I to make sure that I'm able to properly manage stress.\n"
     ]
    }
   ],
   "source": [
    "record_video_and_audio() # record and capture the audio\n",
    "text = transcribe_audio()\n",
    "sentiment = determine_sentiment(text)\n",
    "emotions = analyze_video_emotions()\n",
    "response = chatbot_response(emotions, sentiment, text)\n",
    "print(f\"TherapyAI Response: {response}\\nSentiment: {sentiment}\\nTranscribed Text: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e45cb630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['happy0', 'surprise0']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
