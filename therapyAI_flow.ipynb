{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7636b468",
   "metadata": {},
   "source": [
    "capture audio and video data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88853729",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import pyaudio\n",
    "import wave\n",
    "import time\n",
    "import threading\n",
    "\n",
    "def record_video_and_audio():\n",
    "    cap = cv.VideoCapture(0)\n",
    "    fourcc = cv.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv.VideoWriter('C:\\\\Users\\\\chels\\\\Desktop\\\\COSC490-MER\\\\outputs\\\\video\\\\output.mp4', fourcc, 20.0, (640, 480))\n",
    "\n",
    "    CHUNK = 4096  # Increased chunk size for smoother audio\n",
    "    FORMAT = pyaudio.paInt16\n",
    "    CHANNELS = 1\n",
    "    RATE = 44100\n",
    "    WAVE_OUTPUT_FILENAME = \"C:\\\\Users\\\\chels\\\\Desktop\\\\COSC490-MER\\\\outputs\\\\audio\\\\output.wav\"\n",
    "    frames = []\n",
    "\n",
    "    p = pyaudio.PyAudio()\n",
    "    stream = None\n",
    "\n",
    "    audio_started = False\n",
    "    audio_running = True\n",
    "\n",
    "    def audio_record():\n",
    "        nonlocal stream, frames, audio_running\n",
    "        stream = p.open(format=FORMAT,\n",
    "                        channels=CHANNELS,\n",
    "                        rate=RATE,\n",
    "                        input=True,\n",
    "                        frames_per_buffer=CHUNK)\n",
    "        print(\"* recording audio\")\n",
    "        while audio_running:\n",
    "            data = stream.read(CHUNK, exception_on_overflow=False)\n",
    "            frames.append(data)\n",
    "\n",
    "    audio_thread = threading.Thread(target=audio_record)\n",
    "    audio_thread.start()\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Can't receive frame (stream end?). Exiting ...\")\n",
    "            break\n",
    "        out.write(frame)\n",
    "        cv.imshow('frame', frame)\n",
    "\n",
    "        if cv.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Cleanup video\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv.destroyAllWindows()\n",
    "\n",
    "    # Cleanup audio\n",
    "    audio_running = False\n",
    "    audio_thread.join()\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "    wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "    wf.setnchannels(CHANNELS)\n",
    "    wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(b''.join(frames))\n",
    "    wf.close()\n",
    "    print(\"* done recording audio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4d4bb4",
   "metadata": {},
   "source": [
    "grab the transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "999172fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def transcribe_audio():\n",
    "  base_url = \"https://api.assemblyai.com\"\n",
    "\n",
    "  headers = {\n",
    "      \"authorization\": \"6a35340cac1c443e8e4bbc1d027a3ad5\"\n",
    "  }\n",
    "  with open(\"outputs/audio/output.wav\", \"rb\") as f:\n",
    "    response = requests.post(base_url + \"/v2/upload\",\n",
    "                            headers=headers,\n",
    "                            data=f)\n",
    "\n",
    "  audio_url = response.json()[\"upload_url\"]\n",
    "\n",
    "\n",
    "  data = {\n",
    "      \"audio_url\": audio_url,\n",
    "      \"speech_model\": \"universal\"\n",
    "  }\n",
    "\n",
    "  url = base_url + \"/v2/transcript\"\n",
    "  response = requests.post(url, json=data, headers=headers)\n",
    "\n",
    "  transcript_id = response.json()['id']\n",
    "  polling_endpoint = base_url + \"/v2/transcript/\" + transcript_id\n",
    "\n",
    "  while True:\n",
    "    transcription_result = requests.get(polling_endpoint, headers=headers).json()\n",
    "    transcript_text = transcription_result['text']\n",
    "\n",
    "    if transcription_result['status'] == 'completed':\n",
    "      print(\"Transcription completed.\")\n",
    "      return transcript_text\n",
    "      # with open(\"transcript.txt\", \"w\") as file:\n",
    "      #   file.write(transcript_text) # we don't necessarily need to write it into a file\n",
    "      # file.close()\n",
    "\n",
    "    elif transcription_result['status'] == 'error':\n",
    "      raise RuntimeError(f\"Transcription failed: {transcription_result['error']}\")\n",
    "\n",
    "    else:\n",
    "      time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae605509",
   "metadata": {},
   "source": [
    "sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a611091f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "# analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# def tokens_to_text(tokens):\n",
    "#     text = \" \".join(tokens)\n",
    "#     score = analyzer.polarity_scores(text)\n",
    "#     return score\n",
    "\n",
    "def determine_sentiment(text):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    score = analyzer.polarity_scores(text)\n",
    "    if score['compound'] >= 0.05:\n",
    "        return 'positive'\n",
    "    elif score['compound'] <= -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b47126c",
   "metadata": {},
   "source": [
    "video analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "de457621",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fer.fer import FER\n",
    "from fer.classes import Video\n",
    "import pandas as pd\n",
    "def analyze_video_emotions():\n",
    "    emotion_detector = FER(mtcnn=True)\n",
    "    video = Video(\"C:\\\\Users\\\\chels\\\\Desktop\\\\COSC490-MER\\\\outputs\\\\video\\\\output.mp4\")\n",
    "\n",
    "\n",
    "    emotions = video.analyze(emotion_detector, display=False, frequency=15)\n",
    "    df = pd.DataFrame(emotions)\n",
    "\n",
    "\n",
    "    stats = df.describe()\n",
    "    emotions = stats.loc['mean'].nlargest(2).index.tolist()\n",
    "    return emotions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bd7be6",
   "metadata": {},
   "source": [
    "therapyAI chatbot response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5dc9d07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "def chatbot_response(emotions, sentiment, text):\n",
    "  response: ChatResponse = chat(model='gemma3', messages=[\n",
    "    {\n",
    "      'role': 'system',\n",
    "      'content': f'Based on the user\\'s two primary emotions {emotions[0]} and {emotions[1]} from the video analysis, and based on the sentiment of the text which is {sentiment}, generate a therapuetic, empathetic response to the user\\'s words. Avoid asking follow-up questions (if waranted), but rather provide advice as a therapist would. The user said: {text}'\n",
    "    }\n",
    "  ])\n",
    "  # or access fields directly from the response object\n",
    "  print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a20c65",
   "metadata": {},
   "source": [
    "final call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c8eb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* recording audio\n",
      "* done recording audio\n",
      "Transcription completed.\n"
     ]
    }
   ],
   "source": [
    "record_video_and_audio() # record and capture the audio\n",
    "text = transcribe_audio()\n",
    "sentiment = determine_sentiment(text)\n",
    "chatbot_response(analyze_video_emotions(), sentiment, text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
